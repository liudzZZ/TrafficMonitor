#设置source名称
a.sources = r1
#设置channel的名称
a.channels = c1
#设置sink的名称
a.sinks = k1
#flume-ng agent -n a -f ./a.properties -Dflume.root.logger=INFO,console
# For each one of the sources, the type is defined
#设置source类型为TAILDIR，监控目录下的文件
#Taildir Source可实时监控目录一批文件，并记录每个文件最新消费位置，agent进程重启后不会有重复消费的问题
a.sources.r1.type = TAILDIR
#文件的组，可以定义多种
a.sources.r1.filegroups = f1
#第一组监控的是test1文件夹中的什么文件：.log文件
a.sources.r1.filegroups.f1 = /trafficMonitor/logs/common/.*log
#第二组监控的是test2文件夹中的什么文件：以.txt结尾的文件
#a.sources.r1.filegroups.f2 = /software/logs/system/*.txt


# The channel can be defined as follows.
#设置source的channel名称
a.sources.r1.channels = c1
a.sources.r1.max-line-length = 1000000
#a.sources.r1.eventSize = 512000000

# Each channel's type is defined.
#设置channel的类型
a.channels.c1.type = memory
# Other config values specific to each type of channel(sink or source)
# can be defined as well
# In this case, it specifies the capacity of the memory channel
#设置channel道中最大可以存储的event数量
a.channels.c1.capacity = 1000
#每次最大从source获取或者发送到sink中的数据量
a.channels.c1.transcationCapacity=100

# Each sink's type must be defined
#设置Kafka接收器
a.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
#设置Kafka的broker地址和端口号
a.sinks.k1.brokerList=hadoop101:9092,hadoop102:9092,hadoop103:9092
#设置Kafka的Topic
a.sinks.k1.topic=t_traffic_msb
#设置序列化方式
a.sinks.k1.serializer.class=kafka.serializer.StringEncoder
#Specify the channel the sink should use
#设置sink的channel名称
a.sinks.k1.channel = c1
